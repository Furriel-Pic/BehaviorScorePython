{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -U lime\n",
    "\n",
    "import pandas as pd\n",
    "import numpy  as np\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import statsmodels.api as sm\n",
    "from tabulate import tabulate\n",
    "from texttable import Texttable\n",
    "\n",
    "from plotly.subplots import make_subplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base1 = pd.read_csv(\"C:/Users/jaque/Desktop/Behavior Score/Bases/AmostraBehavior.csv\", sep = \";\")\n",
    "base1[\"Intercept\"] = 1\n",
    "base1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<center><strong><font color = \"darkblue\" size=4> Preparação ABT modelagem </font></strong></center>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = base1.columns[2:len(base1.columns)]\n",
    "\n",
    "X = base1[labels] # Covariáveis\n",
    "y = base1.Target # Target\n",
    "\n",
    "# Base dados balanceados\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.4, random_state = 999)\n",
    "X_val, X_test, y_val, y_test     = train_test_split(X_test, y_test, test_size = 0.5, random_state = 999)\n",
    "\n",
    "print(y_train.count(), y_test.count(),y_val.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<center><strong><font color = \"darkblue\" size=4> Balanceamento da informação</font></strong></center>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from sklearn.datasets import make_classification\n",
    "from imblearn.under_sampling import RandomUnderSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# https://machinelearningmastery.com/random-oversampling-and-undersampling-for-imbalanced-classification/\n",
    "\n",
    "under = RandomUnderSampler(sampling_strategy=0.5)\n",
    "X_under_train, y_under_train = under.fit_resample(X_train, y_train)\n",
    "tab2  = pd.crosstab(index = y_under_train, columns = [\"count\"]) \n",
    "tab2[\"percent\"] = (tab2/tab2.sum())*100\n",
    "tab2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparando as respostas\n",
    "\n",
    "tab_under  = pd.crosstab(index = y_under_train, columns = [\"count\"]) \n",
    "tab_under[\"percent_under\"] = (tab_under/tab_under.sum())*100\n",
    "tab_under = tab_under.reset_index()\n",
    "\n",
    "tab_orig  = pd.crosstab(index = y_train, columns = [\"count\"])\n",
    "tab_orig[\"percent_orig\"] = (tab_orig/tab_orig.sum())*100\n",
    "tab_orig = tab_orig.reset_index()\n",
    "\n",
    "\n",
    "fig_comp = make_subplots(rows=1, cols=2)\n",
    "\n",
    "trace0 = go.Bar(x = [\"Bom\",\"Mau\"], y = tab_under[\"percent_under\"], name = \"Balanceado\", marker_color = \"#0066cc\")\n",
    "trace1 = go.Bar(x = [\"Bom\",\"Mau\"], y = tab_orig[\"percent_orig\"], name = \"DesBalanceado\", marker_color = \"#0099cc\")\n",
    "\n",
    "fig_comp.append_trace(trace0,  1, 1)\n",
    "fig_comp.append_trace(trace1,  1, 2)\n",
    "\n",
    "fig_comp.update_layout(height = 400, width = 900, title = \"Comparação entre target balanceado e desbalanceado\")\n",
    "\n",
    "fig_comp.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_under_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<center><strong><font color = \"darkblue\" size=4> Funções de Avaliação dos modelos </font></strong></center>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn import metrics\n",
    "from scipy.stats import ks_2samp\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descrição das medidas de avaliação\n",
    "\n",
    "TP: Classificação correta da classe Positiva;<br>\n",
    "\n",
    "FN (Erro Tipo II): erro em que o modelo previu a classe Negativo quando o real era Positiva;<br>\n",
    "\n",
    "FP (Erro Tipo I): erro em que o modelo previu a classe Positiva quando o real era Negativa;<br>\n",
    "\n",
    "TN: Classificação correta da classe Negativa.<br>\n",
    "<br>\n",
    "\n",
    "- Acurácia [0,1]: É a proporção de predições corretas, sem considerar o que é positivo e o que negativo e sim o acerto total. É dada por: TP (Verdadeiro positivo) + TN (Verdadeiro negativo). \n",
    "<br>\n",
    "\n",
    "- Precisão [0,1]: Do total de classificados positivos, quantos efetivamente eram: TP/(FP+TP), ou seja, quanto do evento de interesse estou acertando.\n",
    "<br>\n",
    "\n",
    "- Recall [0,1]: Do total de observados que são meu evento de interessem quantos efetivamente são: TP/(FN+TP), isto é, quando o modelo deve de qualquer maneira encontrar o evento de interesse, mesmo que classifique alguns não eventos como positivos (situação de Falso Positivo) no processo, deve-se considerar tal medida. \n",
    "<br>\n",
    "\n",
    "- AUC [0,1]: O AUC representa a área da crva ROC e mede a capacidade de separabilidade do modelo. Quanto maior o AUC, melhor o modelo está em prever 0 como 0 e 1 como 1. \n",
    "<br>\n",
    "\n",
    "- Gini [0,1]: O Coeficiente de Gini consiste em um número entre 0 e 1, e que 0 corresponde à completa igualdade e 1 corresponde à completa desigualdade entre os eventos comparados, sendo obtido por Gini = 2*AUC-1; \n",
    "<br>\n",
    "\n",
    "- LogLoss [-inf,1]: A perda logarítmica (relacionada à entropia cruzada) mede o desempenho de um modelo de classificação em que a entrada de previsão é um valor de probabilidade entre 0 e 1. Verificando a vantagem do classificador sobre uma previsão aleatória. Logloss = [-1/n] sum(n)sum(m)[y*log(p)]\n",
    "<br>\n",
    "\n",
    "- KS [0,1]: Técnica não paramétrica para determinar se duas amostras foram extraídas de populações com distribuições similares. Dada por KS = max|F0(y) - F1(y)|.\n",
    "<br>\n",
    "\n",
    "- ASE: Média dos erros de predição ao quadrado, a medida não tem interpretação direta sozinha, mas pode ser utilizada para comparação de modelos, sendo o modelos com menor ASE, o que apresenta o menor erro médio de classificação. \n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para diagnóstico dos modelos\n",
    "#https://stackoverflow.com/questions/53846943/sklearn-logistic-regression-adjust-cutoff-point\n",
    "\n",
    "def AVALIA_MOD(modelo, y_prob, y_obs, titulo, base, corte):\n",
    "    \n",
    "    pred_Y_test = {}\n",
    "    pred_Y_data = pd.DataFrame(modelo.predict_proba(y_prob)[:,1], columns=[\"y_prob\"])\n",
    "    pred_Y_data[\"y_hat\"] = modelo.predict(y_prob)\n",
    "    obs_Y_data = y_obs.reset_index()\n",
    "    pred_data = pd.concat([obs_Y_data,\n",
    "                         pred_Y_data], axis = 1)\n",
    "    \n",
    "    pred_data[\"erro2\"]     = (pred_data.Target - pred_data.y_prob)**2\n",
    "    pred_data[\"y_hat_new\"] = np.where(pred_data.y_prob > corte, 1, 0)\n",
    "    \n",
    "    print(\"--------------------------------------------\")\n",
    "    print(\"----------------\",base,\"--------------------\")\n",
    "    print(\"\\n\")\n",
    "    KS = ks_2samp(pred_data.loc[pred_data.Target == 0, \"y_prob\"], pred_data.loc[pred_data.Target == 1, \"y_prob\"])\n",
    "    KS = round(KS[0],2)*100\n",
    "    \n",
    "    ASE = round(np.mean(pred_data.erro2),2)\n",
    "    \n",
    "    AUC = roc_auc_score(pred_data.Target,pred_data.y_prob)\n",
    "    GINI = 2*AUC - 1\n",
    "    AUC = round(AUC, 2)*100\n",
    "\n",
    "    # Curva Roc\n",
    "    fpr, tpr, thresholds  = roc_curve(pred_data.Target,pred_data.y_prob)\n",
    "    \n",
    "    trace1 = go.Scatter(x = fpr, y = tpr, \n",
    "                        mode = \"lines\", \n",
    "                        line = dict(color = \"#b30000\", width = 2),\n",
    "                        name = 'AUC: %0.2f' % AUC)\n",
    "    trace2 = go.Scatter(x=[0, 1], y=[0, 1], \n",
    "                        mode='lines', \n",
    "                        line=dict(color = \"navy\", width=2, dash = \"dash\"),\n",
    "                        showlegend=False)\n",
    "    layout = go.Layout(xaxis=dict(title = \"FP\"),\n",
    "                       yaxis=dict(title = \"TP\"))\n",
    "    plot_roc = go.Figure(data=[trace1, trace2], layout=layout)\n",
    "    plot_roc.update_layout(title_text = \"Curva ROC \"+titulo)\n",
    "    \n",
    "    # Matriz de confusão\n",
    "    cm = confusion_matrix(pred_data.Target, pred_data.y_hat)\n",
    "    conf_m = cm/np.sum(cm)*100\n",
    "    TP = round(conf_m[0,0],2)\n",
    "    FP = round(conf_m[0,1],2)\n",
    "    FN = round(conf_m[1,0],2)\n",
    "    TN = round(conf_m[1,1],2)\n",
    "    \n",
    "    cm2 = confusion_matrix(pred_data.Target, pred_data.y_hat_new)\n",
    "    conf_m2 = cm2/np.sum(cm2)*100\n",
    "    acur_cut = conf_m2[0,0] +conf_m2[1,1]\n",
    "    \n",
    "    print(\"Matriz de confusão default\",\n",
    "         \"\\n\",\"|\",\"TP\", TP,\"|\",\"FP\",FP,\"|\",\"\\n\",\"|\",\"FN\", FN,\"|\",\"TN\", TN,\"|\")\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    print(\"Matriz de confusão novo cutoff\",\n",
    "         \"\\n\", conf_m2)\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    CONCORD = round(TP + TN ,2)\n",
    "    PRECS   = TP/(FP+TP)\n",
    "    RECALL  = TP/(FN+TP)\n",
    "                           \n",
    "    LogLoss = log_loss(pred_data.Target, pred_data.y_hat)\n",
    "    \n",
    "    d = {\"Metricas\": [\"Acurácia\",\"Precisão\",\"Recall\",\"LogLoss\",\"AUC\",\"KS\",\"Gini\",\"ASE\"],\n",
    "         \"Valores\": [CONCORD,PRECS,RECALL,LogLoss,AUC, KS,GINI,ASE]}\n",
    "    \n",
    "    # Base de medidas de qualidade dos ajuste\n",
    "    METRICAS = pd.DataFrame(data=d)\n",
    "    \n",
    "    # Base para gráfico de ordenação\n",
    "    pred_data = pred_data.sort_values(by=[\"y_prob\"])\n",
    "\n",
    "    pred_data[\"FX_yprob\"] = pd.qcut(pred_data.y_prob, 10)\n",
    "    pred_data[\"Default\"]  = np.mean(pred_data.Target)\n",
    "\n",
    "    ordena = pred_data.groupby(\"FX_yprob\")[[\"Target\",\"Default\"]].mean().reset_index()   \n",
    "    ordena[\"FX_yprob2\"] = lab = np.arange(10)\n",
    "    ordena[\"LIFT\"] = ordena.Target/ordena.Default\n",
    "\n",
    "#--------------------------------------------------------   \n",
    "    lab1 = titulo[7:]\n",
    "\n",
    "    if lab1 == \"WOE Balanc\":\n",
    "        col = \"#5c5c8a\"\n",
    "    elif lab1 == \"WOE Desbalanc\":\n",
    "        col = \"#cc0000\"\n",
    "    elif lab1 == \"Desbalanc\":\n",
    "        col = \"#008ae6\"     \n",
    "    else:\n",
    "        col = \"#8a8a5c\" \n",
    "#----------------------------------------------------------    \n",
    "    # Gráfico de ordenação\n",
    "    fig_ord = data=go.Scatter(x = ordena.FX_yprob2, y = ordena.LIFT, name = \"Lift \"+titulo, marker=dict(color = col))\n",
    "#    fig_ord.update_layout(title_text = \"Ordenação: Lift \"+titulo)\n",
    "    \n",
    "    print(\"Acurácia:\", CONCORD,\"Acurácia cutoff:\",round(acur_cut,2),\"|\",\"Precisão:\", round(PRECS,2),\"|\",\"Recall:\", round(RECALL,2))\n",
    "    print(\"\\n\")\n",
    "    print(\"KS:\", KS,\"|\", \"AUC:\",AUC,\"|\",\"GINI:\",round(GINI,2), \"|\",\"LogLoss:\",round(LogLoss,2), \"|\",\"ASE:\", ASE)\n",
    "    print(\"\\n\")\n",
    "    return(pred_data,ordena,METRICAS,fig_ord,plot_roc,titulo,lab1,AUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para avaliar importância das variáveis/features\n",
    "def ML_IMPORTANCIA(modelo, feat):\n",
    "    importances = modelo.feature_importances_\n",
    "    indices = np.argsort(importances)\n",
    "    indices = pd.Series(indices)\n",
    "    features = pd.Series(feat)\n",
    "    \n",
    "    print(\"    Results: Importância das Variáveis\")\n",
    "    print(\"============================================\")\n",
    "    tt = pd.Series(importances[indices],\n",
    "                 index=features[indices]).sort_values(ascending=False)\n",
    "    print(tt)\n",
    "    print(\"============================================\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para calcular o shift da AUC, ou seja, avaliar se ocorreram quedas na medida em amostras distintas\n",
    "def SHIFT_AUC(x,y):\n",
    "    shift = abs(round((x[7] - y[7])/y[7],2)*100)\n",
    "    z = list(x)\n",
    "    z[2] = x[2].append({\"Metricas\" : \"Shift AUC\" , \"Valores\" : shift} , ignore_index=True)\n",
    "    x = tuple(z)\n",
    "    return(x)\n",
    "    print(\"Shift AUC:\",shift)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<center><strong><font color = \"darkblue\" size=4> Regressão Logística </font></strong></center>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import statsmodels.api as sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# http://prorum.com/?qa=3254/classificacao-usando-stepwise-logistic-regression-usando\n",
    "# https://www.kdnuggets.com/2018/06/step-forward-feature-selection-python.html\n",
    "base1.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><strong><font color = \"darkblue\" size=4>------------------------------------------ Logit: Balanceados por WOE ------------------------------------------</font></strong></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Balanceados por WOE\n",
    "X_WOE2 = [\"Intercept\",\"WOE_CAT_Idade\",\"WOE_N_Atraso60_89Dias\",\"WOE_N_atrasos_Ult90Dias\",\n",
    "          \"WOE_CAT_UltPercLimit\",\"WOE_N_emprestimos\"]\n",
    "\n",
    "# https://www.statsmodels.org/stable/glm.html\n",
    "M_logit_under_WOE  = sm.GLM(y_under_train,  X_under_train[X_WOE2], family = sm.families.Binomial(), ).fit()\n",
    "M_logit_under_WOE2 = LogisticRegression().fit(X_under_train[X_WOE2[1:6]], y_under_train)\n",
    "\n",
    "print(M_logit_under_WOE.summary2())\n",
    "\n",
    "# Modelo balanceado com WOE\n",
    "logit_under_WOE_test = AVALIA_MOD(modelo = M_logit_under_WOE2, y_prob = X_test[X_WOE2[1:6]], y_obs = y_test, base = \"Teste\", titulo = \"Logit: WOE Balanc\", corte = 0.3)\n",
    "logit_under_WOE_val  = AVALIA_MOD(modelo = M_logit_under_WOE2, y_prob = X_val[X_WOE2[1:6]], y_obs = y_val,  base = \"Valida\" , titulo = \"Logit: WOE Balanc\", corte = 0.3)\n",
    "    \n",
    "logit_under_WOE_test = SHIFT_AUC(x = logit_under_WOE_test, y = logit_under_WOE_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><strong><font color = \"darkblue\" size=4> ------------------------------------------ Logit: Não Balanceados por WOE --------------------------------------</font></strong></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Não Balanceados por WOE\n",
    "\n",
    "M_logit_WOE  = sm.GLM(y_train,  X_train[X_WOE2], family=sm.families.Binomial()).fit()\n",
    "M_logit_WOE2 = LogisticRegression().fit(X_train[X_WOE2[1:6]], y_train)\n",
    "\n",
    "print(M_logit_WOE.summary2())\n",
    "\n",
    "# Modelo Não balanceado com WOE\n",
    "\n",
    "logit_WOE_test = AVALIA_MOD(modelo = M_logit_WOE2, y_prob = X_test[X_WOE2[1:6]],  y_obs = y_test, base=\"Teste\", titulo = \"Logit: WOE Desbalanc\",  corte = np.mean(y_test))\n",
    "logit_WOE_val  = AVALIA_MOD(modelo = M_logit_WOE2, y_prob = X_val[X_WOE2[1:6]], y_obs = y_val, base=\"Valida\", titulo = \"Logit: WOE Desbalanc\",corte = np.mean(y_val))\n",
    "\n",
    "logit_WOE_test = SHIFT_AUC(x = logit_WOE_test, y = logit_WOE_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><strong><font color = \"darkblue\" size=4> ------------------------------------------ Logit: Balanceados por Contínuas -------------------------------------- </font></strong></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo Balanceado covariáveis continuas\n",
    "# https://stackoverflow.com/questions/52670012/convergencewarning-liblinear-failed-to-converge-increase-the-number-of-iterati\n",
    "\n",
    "XX = [\"Intercept\",'UltPercLimit', 'Idade', 'N_Atraso30_59Dias', 'N_EmeprestimosAbertos',\n",
    "                'N_atrasos_Ult90Dias', 'N_Atraso60_89Dias']\n",
    "\n",
    "M_logit_under = sm.GLM(y_under_train,  X_under_train[XX], family=sm.families.Binomial(),max_iter = 5000).fit()\n",
    "M_logit_under2 = LogisticRegression().fit(X_under_train[XX[1:6]], y_under_train)\n",
    "\n",
    "print(M_logit_under.summary2())\n",
    "\n",
    "# Modelo  balanceado com Contínuas\n",
    "logit_under_test = AVALIA_MOD(modelo = M_logit_under2, y_prob = X_test[XX[1:6]], y_obs = y_test, base=\"Teste\", titulo = \"Logit: Balanc\", corte = np.mean(y_test))\n",
    "logit_under_val   = AVALIA_MOD(modelo = M_logit_under2, y_prob = X_val[XX[1:6]], y_obs = y_val, base=\"Valida\", titulo = \"Logit: Balanc\", corte = np.mean(y_val))\n",
    "\n",
    "logit_under_test = SHIFT_AUC(x = logit_under_test, y = logit_under_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><strong><font color = \"darkblue\" size=4> ------------------------------------------ Logit: Não Balanceados por Contínuas -------------------------------------- </font></strong></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo Não Balanceado covariáveis continuas\n",
    "\n",
    "M_logit = sm.GLM(y_train,  X_train[XX], family=sm.families.Binomial(),max_iter = 5000).fit()\n",
    "M_logit2 = LogisticRegression().fit(X_train[XX[1:6]], y_train)\n",
    "\n",
    "print(M_logit.summary2())\n",
    "\n",
    "# Modelo balanceado com Contínuas\n",
    "pred_Y_test = {}\n",
    "logit_test = AVALIA_MOD(modelo = M_logit2, y_prob = X_test[XX[1:6]], y_obs = y_test, titulo = \"Logit: Desbalanc\", base=\"Teste\",corte = np.mean(y_test))\n",
    "logit_val  = AVALIA_MOD(modelo = M_logit2, y_prob = X_val[XX[1:6]], y_obs = y_val, titulo = \"Logit: Desbalanc\", base=\"Treino\",corte = np.mean(y_val))\n",
    "\n",
    "logit_test = SHIFT_AUC(x = logit_test, y = logit_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<center><strong><font color = \"darkblue\" size=4> Random Forest </font></strong></center>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://towardsdatascience.com/random-forest-in-python-24d0893d51c0\n",
    "#https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
    "\n",
    "features = ['UltPercLimit', 'Idade', 'N_Atraso30_59Dias',\n",
    "       'RendaMensal', 'N_EmeprestimosAbertos',\n",
    "       'N_atrasos_Ult90Dias', 'N_emprestimos', 'N_Atraso60_89Dias',\n",
    "       'N_dependentes', 'lnRazaoGastos']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><strong><font color = \"darkblue\" size=4> -----------------  RF: Seleção de Hiperparâmetros -------------------</font></strong></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principais Parâmetros do Radom Forest\n",
    "\n",
    "- n_estimators: O número de árvores empregadas no algoritmo\n",
    "<br>\n",
    "\n",
    "- max_features: O número de variáveis amostradas em cada árvore, normalmente usamos sqrt(p), log2(p)\n",
    "<br>\n",
    "\n",
    "- max_depth: É a profundidade da árvore, quanto maior seu valor mais complexa é a árvore, podendo decorar o conjunto de treino (overfitting), o que pode degradar seu poder preditivo quando aplicado a novos dados. Isso pode ser mitigado \"podando\" a árvore de decisão ao atribuir uma profundidade máxima ou uma quantidade máxima de folhas.\n",
    "<br>\n",
    "\n",
    "- min_samples_split: O volume mínimo considerado antes de dividir um nó interno\n",
    "<br>\n",
    "\n",
    "- min_samples_leaf: O volume mínimo de dados permitidos em uma folha\n",
    "<br>\n",
    "\n",
    "- bootstrap: Método de amostragem das linhas(população). Se False, o conjunto de dados inteiro será usado para criar cada árvore, o que pode gerar um problema de árvores altamento correlacionadas.\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 900, num = 6)]\n",
    "\n",
    "max_features = ['auto', 'sqrt']\n",
    "\n",
    "max_depth = [int(x) for x in np.linspace(10, 100, num = 5)]\n",
    "max_depth.append(None)\n",
    "\n",
    "min_samples_split = [50, 200, 500]\n",
    "\n",
    "min_samples_leaf = [10, 20, 50]\n",
    "\n",
    "bootstrap = [True]\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "\n",
    "random_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando o modelo base\n",
    "rf = RandomForestClassifier()\n",
    "# Utilizando 3 fold cross validation, \n",
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, \n",
    "                               cv = 3, verbose=2, random_state=999, n_jobs = -1)\n",
    "# Fit the random search model\n",
    "rf_random.fit(X_under_train[features], y_under_train)\n",
    "rf_random.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><strong><font color = \"darkblue\" size=4> -----------------  RF: Dados contínuos Não balanceados -------------------</font></strong></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators = 480, random_state = 999, min_samples_split = 10, \n",
    "                            min_samples_leaf = 4, max_features = \"auto\", max_depth = 10, \n",
    "                                bootstrap = True).fit(X_train[features], y_train)\n",
    "\n",
    "ML_IMPORTANCIA(rf, features)\n",
    "\n",
    "# Modelo Não balanceado com Contínuas\n",
    "rf_test = AVALIA_MOD(modelo = rf, y_prob = X_test[features], y_obs = y_test, titulo = \"Randf: Desbalanc\", base=\"Teste\", corte = np.mean(y_test))\n",
    "rf_val  = AVALIA_MOD(modelo = rf, y_prob = X_val[features],  y_obs = y_val,  titulo = \"Randf: Desbalanc\",  base=\"Valida\", corte = np.mean(y_val))\n",
    "\n",
    "rf_test = SHIFT_AUC(x = rf_test, y = rf_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><strong><font color = \"darkblue\" size=4> -----------------  RF: Dados contínuos balanceados -------------------</font></strong></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_under = RandomForestClassifier(n_estimators = 480, random_state = 999, min_samples_split = 10, \n",
    "                            min_samples_leaf = 4, max_features = \"auto\", max_depth = 10, \n",
    "                                bootstrap = True).fit(X_under_train[features], y_under_train)\n",
    "\n",
    "ML_IMPORTANCIA(rf_under, features)\n",
    "\n",
    "# Modelo Não balanceado com Contínuas\n",
    "rf_test_under = AVALIA_MOD(modelo = rf_under, y_prob = X_test[features], y_obs = y_test, titulo = \"Randf: Balanc\", base=\"Teste\", corte = 0.3)\n",
    "rf_val_under  = AVALIA_MOD(modelo = rf_under, y_prob = X_val[features],  y_obs = y_val,  titulo = \"Randf: Balanc\", base=\"Valida\", corte = 0.3)\n",
    "\n",
    "rf_test_under = SHIFT_AUC(x = rf_test_under, y = rf_val_under)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><strong><font color = \"darkblue\" size=4> -----------------  RF: Dados WOE Não balanceados -------------------</font></strong></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_WOE3 = [\"WOE_CAT_Idade\",\"WOE_N_Atraso60_89Dias\",\"WOE_N_atrasos_Ult90Dias\",\n",
    "          \"WOE_CAT_UltPercLimit\",\"WOE_N_emprestimos\"]\n",
    "    \n",
    "rf_WOE = RandomForestClassifier(n_estimators = 480, random_state = 999, min_samples_split = 10, \n",
    "                            min_samples_leaf = 4, max_features = \"auto\", max_depth = 10, \n",
    "                                bootstrap = True).fit(X_train[X_WOE3], y_train)\n",
    "\n",
    "ML_IMPORTANCIA(rf_WOE, X_WOE3)\n",
    "\n",
    "# Modelo Não balanceado com Contínuas\n",
    "rf_test_WOE = AVALIA_MOD(modelo = rf_WOE, y_prob = X_test[X_WOE3], y_obs = y_test, titulo = \"Randf: WOE Desbalanc\", base=\"Teste\",corte = np.mean(y_test))\n",
    "rf_val_WOE  = AVALIA_MOD(modelo = rf_WOE, y_prob = X_val[X_WOE3],  y_obs = y_val, titulo = \"Randf: WOE Desbalanc\",  base=\"Valida\",corte = np.mean(y_val))\n",
    "\n",
    "rf_test_WOE = SHIFT_AUC(x = rf_test_WOE, y = rf_val_WOE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><strong><font color = \"darkblue\" size=4> -----------------  RF: Dados WOE balanceados -------------------</font></strong></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rf_WOE_under = RandomForestClassifier(n_estimators = 480, random_state = 999, min_samples_split = 10, \n",
    "                            min_samples_leaf = 4, max_features = \"auto\", max_depth = 10, \n",
    "                                bootstrap = True).fit(X_under_train[X_WOE3], y_under_train)\n",
    "\n",
    "ML_IMPORTANCIA(rf_WOE_under, X_WOE3)\n",
    "\n",
    "# Modelo Não balanceado com Contínuas\n",
    "rf_test_under_WOE = AVALIA_MOD(modelo = rf_WOE, y_prob = X_test[X_WOE3], y_obs = y_test, titulo = \"Randf: WOE Balanc\", base=\"Teste\",corte = 0.3)\n",
    "rf_val_under_WOE  = AVALIA_MOD(modelo = rf_WOE, y_prob = X_val[X_WOE3],  y_obs = y_val, titulo = \"Randf: WOE Balanc\",  base=\"Valida\",corte = 0.3)\n",
    "\n",
    "rf_test_under_WOE = SHIFT_AUC(x = rf_test_under_WOE, y = rf_val_under_WOE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<center><strong><font color = \"darkblue\" size=4> Gradiet boosting </font></strong></center>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principais Parâmetros do Gradient Boosting\n",
    "\n",
    "- learning_rate: Determina o impacto de cada árvore no resultado final. O GBM funciona com uma estimativa inicial que é atualizada usando a saída de cada árvore. O parâmetro de learning controla a magnitude dessa mudança nas estimativas. Valores mais baixos são geralmente preferidos, pois tornam o modelo robusto às características específicas da árvore e, assim, permite que ele se generalize bem. Valores mais baixos exigem um número maior de árvores para modelar todas as relações sendo computacionalmente exaustivos.\n",
    "<br>\n",
    "\n",
    "- n_estimators: O número de árvores sequenciais a serem modeladas. O GB é bastante robusto contra o overfitting, portanto um grande número geralmente resulta em melhor desempenho.\n",
    "<br>\n",
    "\n",
    "- max_depth: LTime_serieimita o número de nós na árvore. Ajuste esse parâmetro para obter o melhor desempenho; o melhor valor depende da interação das variáveis de entrada\n",
    "<br>\n",
    "\n",
    "- subsample: A parcela de observações selecionadas para cada árvore. A seleção é feita por amostragem aleatória. Valores ligeiramente inferiores a 1 tornam o modelo robusto, reduzindo a variação. Valores próximos a 0.8 geralmente funcionam bem.\n",
    "<br>\n",
    "\n",
    "- loss:  Refere-se à função de perda a ser minimizada em cada divisão. Geralmente os valores default funcionam bem. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><strong><font color = \"darkblue\" size=4> -----------------  GB: Seleção de Hiperparâmetros -------------------</font></strong></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.datacareer.de/blog/parameter-tuning-in-gradient-boosting-gbm/\n",
    "\n",
    "test3 = { 'learning_rate':[0.1, 0.05, 0.01, 0.005], \n",
    "            'n_estimators':[100, 250, 500, 1000, 1500],\n",
    "            'max_depth':[2, 3, 5, 6] }\n",
    "\n",
    "tuning = GridSearchCV(estimator = GradientBoostingClassifier(min_samples_split = 2, min_samples_leaf = 1, \n",
    "                                                             subsample = 1, max_features = \"sqrt\", random_state = 999), \n",
    "            param_grid = test3, scoring = \"accuracy\", n_jobs = 4, iid = False, cv = 5)\n",
    "\n",
    "tuning.fit(X_under_train[features], y_under_train)\n",
    "tuning.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><strong><font color = \"darkblue\" size=4> -----------------  GB: Dados contínuos Balanceados -------------------</font></strong></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html\n",
    "\n",
    "clf_under = GradientBoostingClassifier(learning_rate = 0.01, max_depth = 3, n_estimators = 1000,\n",
    "                                       random_state = 999).fit(X_under_train[features], y_under_train)\n",
    "\n",
    "ML_IMPORTANCIA(clf_under, features)\n",
    "\n",
    "# Balanceados contínuos\n",
    "gb_test_under = AVALIA_MOD(modelo = clf_under, y_prob = X_test[features], y_obs = y_test, titulo = \"GBoos: Balanc\", base = \"Teste\", corte = 0.3)\n",
    "gb_val_under  = AVALIA_MOD(modelo = clf_under, y_prob = X_val[features],  y_obs = y_val,  titulo = \"GBoos: Balanc\", base = \"Valida\", corte = 0.3)\n",
    "\n",
    "gb_test_under = SHIFT_AUC(x = gb_test_under, y = gb_val_under)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><strong><font color = \"darkblue\" size=4> -----------------  GB: Dados contínuos Não Balanceados -------------------</font></strong></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = GradientBoostingClassifier(learning_rate = 0.01, max_depth = 3, n_estimators = 1000,\n",
    "                                       random_state = 999).fit(X_train[features], y_train)\n",
    "\n",
    "ML_IMPORTANCIA(clf, features)\n",
    "\n",
    "# Não balanceados contínuos\n",
    "gb_test = AVALIA_MOD(modelo = clf, y_prob = X_test[features], y_obs = y_test, titulo = \"GBoos: Desbalanc\", base = \"Teste\", corte = 0.3)\n",
    "gb_val  = AVALIA_MOD(modelo = clf, y_prob = X_val[features],  y_obs = y_val,  titulo = \"GBoos: Desbalanc\", base = \"Valida\", corte = 0.3)\n",
    "\n",
    "gb_test = SHIFT_AUC(x = gb_test, y = gb_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><strong><font color = \"darkblue\" size=4> -----------------  GB: Dados WOE Não Balanceados -------------------</font></strong></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_woe = GradientBoostingClassifier(learning_rate = 0.01, max_depth = 3, n_estimators = 1000,\n",
    "                                       random_state = 999).fit(X_train[X_WOE3], y_train)\n",
    "\n",
    "ML_IMPORTANCIA(clf_woe, X_WOE3)\n",
    "\n",
    "# Não balanceados WOE\n",
    "gb_WOE_test = AVALIA_MOD(modelo = clf_woe, y_prob = X_test[X_WOE3], y_obs = y_test, titulo = \"GBoos: WOE Desbalanc\", base = \"Teste\", corte = 0.3)\n",
    "gb_WOE_val  = AVALIA_MOD(modelo = clf_woe, y_prob = X_val[X_WOE3],  y_obs = y_val,  titulo = \"GBoos: WOE Desbalanc\", base = \"Valida\", corte = 0.3)\n",
    "\n",
    "gb_WOE_test = SHIFT_AUC(x = gb_WOE_test, y = gb_WOE_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><strong><font color = \"darkblue\" size=4> -----------------  GB: Dados WOE Balanceados -------------------</font></strong></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "clf_woe_under = GradientBoostingClassifier(learning_rate = 0.01, max_depth = 3, n_estimators = 1000,\n",
    "                                       random_state = 999).fit(X_under_train[X_WOE3], y_under_train)\n",
    "\n",
    "ML_IMPORTANCIA(clf_woe_under, X_WOE3)\n",
    "\n",
    "# Não balanceados WOE\n",
    "gb_under_WOE_test = AVALIA_MOD(modelo = clf_woe_under, y_prob = X_test[X_WOE3], y_obs = y_test, titulo = \"GBoos: WOE Balanc\", base = \"Teste\", corte = 0.3)\n",
    "gb_under_WOE_val  = AVALIA_MOD(modelo = clf_woe_under, y_prob = X_val[X_WOE3],  y_obs = y_val,  titulo = \"GBoos: WOE Balanc\", base = \"Valida\", corte = 0.3)\n",
    "\n",
    "gb_under_WOE_test = SHIFT_AUC(x = gb_under_WOE_test, y = gb_under_WOE_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<center><strong><font color = \"darkblue\" size=4> Adaboost </font></strong></center>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principais Parâmetros do AdaBoost\n",
    "\n",
    "- learning_rate: Taxa de aprendizado que controla a contribuição de cada modelo para a previsão do conjunto. Por padrão, é definido como 1 ou contribuição total. Valores menores ou maiores podem ser adequados, dependendo do número de modelos usados no conjunto. É preciso ter um equilíbrio entre a contribuição dos modelos e o número de árvores no conjunto.\n",
    "<br>\n",
    "\n",
    "- n_estimators: Como as árvores de decisão usadas no algoritmo são simples. O número de árvores adicionadas ao modelo deve ser alto funcione bem. O número de árvores o padrão é 50.\n",
    "<br>\n",
    "\n",
    "- base_estimator: Define o tipo de algoritmo utilizado no AdaBoost, o default é DecisionTreeClassifier(max_depth=1). Contudo é possível utilizarLogisticRegression() ou KNeighborsClassifier ()."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><strong><font color = \"darkblue\" size=4> -----------------  ADA: Seleção de Hiperparâmetros -------------------</font></strong></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://educationalresearchtechniques.com/2019/01/02/adaboost-classification-in-python/\n",
    "\n",
    "test3 = { 'learning_rate':[0.1, 0.05, 0.01], \n",
    "            'n_estimators':[100, 250, 500, 1000,1500] }\n",
    "\n",
    "ada    = AdaBoostClassifier()\n",
    "search = GridSearchCV(estimator = ada, param_grid = test3, scoring = 'accuracy', n_jobs = 1, cv = 5)\n",
    "search.fit(X_under_train[features], y_under_train)\n",
    "search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><strong><font color = \"darkblue\" size=4> -----------------  ADA: Dados Balanceados -------------------</font></strong></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.datacamp.com/community/tutorials/adaboost-classifier-python?utm_source=adwords_ppc&utm_campaignid=10267161064&utm_adgroupid=102842301792&utm_device=c&utm_keyword=&utm_matchtype=b&utm_network=g&utm_adpostion=&utm_creative=278443377086&utm_targetid=aud-522010995285:dsa-429603003980&utm_loc_interest_ms=&utm_loc_physical_ms=1001634&gclid=Cj0KCQjwgJv4BRCrARIsAB17JI76HKOOHYyhkIbR3eQUd9yfERhL7QoLCSehhj1eeEyyOV0cmVb5pRcaAlpnEALw_wcB\n",
    "\n",
    "ada_under = AdaBoostClassifier(n_estimators = 500, learning_rate = 0.1).fit(X_under_train[features], y_under_train)\n",
    "\n",
    "ML_IMPORTANCIA(ada_under, features)\n",
    "\n",
    "# Train Adaboost Classifer\n",
    "ada_test_under = AVALIA_MOD(modelo = ada_under, y_prob = X_test[features], y_obs = y_test, titulo = \"AdaBo: Balanc\", base = \"Teste\", corte =0.3)\n",
    "ada_val_under  = AVALIA_MOD(modelo = ada_under, y_prob = X_val[features],  y_obs = y_val,  titulo = \"AdaBo: Balanc\", base = \"Valida\", corte = 0.3)\n",
    "\n",
    "ada_test_under = SHIFT_AUC(x = ada_test_under, y = ada_val_under)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><strong><font color = \"darkblue\" size=4> -----------------  ADA: Dados Não Balanceados -------------------</font></strong></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada = AdaBoostClassifier(n_estimators=50,\n",
    "                         learning_rate=1).fit(X_train[features], y_train)\n",
    "\n",
    "ML_IMPORTANCIA(ada, features)\n",
    "\n",
    "# Train Adaboost Classifer\n",
    "ada_test = AVALIA_MOD(modelo = ada, y_prob = X_test[features], y_obs = y_test, titulo = \"AdaBo: Desbalanc\", base = \"Teste\", corte = 0.3)\n",
    "ada_val  = AVALIA_MOD(modelo = ada, y_prob = X_val[features],  y_obs = y_val,  titulo = \"AdaBo: Desbalanc\", base = \"Valida\", corte = 0.3)\n",
    "\n",
    "ada_test = SHIFT_AUC(x = ada_test, y = ada_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><strong><font color = \"darkblue\" size=4> -----------------  ADA: Dados WOE Balanceados -------------------</font></strong></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada_WOE_under = AdaBoostClassifier(n_estimators = 500, learning_rate = 0.1).fit(X_under_train[X_WOE3], y_under_train)\n",
    "\n",
    "ML_IMPORTANCIA(ada_WOE_under, X_WOE3)\n",
    "\n",
    "# Train Adaboost Classifer\n",
    "ada_test_WOE_under = AVALIA_MOD(modelo = ada_WOE_under, y_prob = X_test[X_WOE3], y_obs = y_test, titulo = \"AdaBo: WOE Balanc\", base = \"Teste\", corte = 0.5)\n",
    "ada_val_WOE_under  = AVALIA_MOD(modelo = ada_WOE_under, y_prob = X_val[X_WOE3],  y_obs = y_val,  titulo = \"AdaBo: WOE Balanc\", base = \"Valida\", corte = 0.5)\n",
    "\n",
    "ada_test_WOE_under = SHIFT_AUC(x = ada_test_WOE_under, y = ada_val_WOE_under)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><strong><font color = \"darkblue\" size=4> -----------------  ADA: Dados WOE Não Balanceados -------------------</font></strong></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada_WOE = AdaBoostClassifier(n_estimators=50,\n",
    "                         learning_rate=1).fit(X_train[X_WOE3], y_train)\n",
    "\n",
    "ML_IMPORTANCIA(ada_WOE, X_WOE3)\n",
    "\n",
    "# Train Adaboost Classifer\n",
    "ada_WOE_test = AVALIA_MOD(modelo = ada_WOE, y_prob = X_test[X_WOE3], y_obs = y_test, titulo = \"AdaBo: WOE Desbalanc\", base = \"Teste\", corte = 0.5)\n",
    "ada_WOE_val  = AVALIA_MOD(modelo = ada_WOE, y_prob = X_val[X_WOE3],  y_obs = y_val,  titulo = \"AdaBo: WOE Desbalanc\", base = \"Valida\", corte = 0.5)\n",
    "\n",
    "ada_WOE_test = SHIFT_AUC(x = ada_WOE_test, y = ada_WOE_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<center><strong><font color = \"darkblue\" size=4> Redes Neurais </font></strong></center>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><strong><font color = \"darkblue\" size=4> -----------------  Redes Neurais: Ajuste hiperparâmetros -------------------</font></strong></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principais Parâmetros das Redes Neurais\n",
    "\n",
    "- hidden_layer_sizes: Nos permite definir o número de camadas e o número de nós que desejamos ter. Cada elemento da tupla representa o número de nós na i-ésima posição em que i é o índice da tupla. Assim, o comprimento da tupla indica o número total de camadas ocultas na rede.\n",
    "<br>\n",
    "\n",
    "- activation: As funções de ativação introduzem o componente não linear nas redes neurais, que faz com que elas possam aprender mais do que relações lineares entre as variáveis dependentes e independentes, temos:\n",
    "    - logistic, função sigmóide ou logística, retorna f(x) = 1/(1 + exp (-x))\n",
    "    - tanh, função hiperbólica, retorna f(x) = tanh (x).\n",
    "    - relu, retorna f(x) = max (0, x)\n",
    "<br>\n",
    "\n",
    "- solver: Especifica o algoritmo de otimização de peso entre os nós.\n",
    "<br>\n",
    "\n",
    "- alpha: É um parâmetro de regularização, também conhecido como termo de penalidade, que combate o excesso de ajuste restringindo o tamanho dos pesos. O aumento do alfa pode corrigir o overfitting, resultando em menor capacidade de molde aos dados. Da mesma forma, a diminuição do alpha pode corrigir um viés alto (underfitting), incentivando pesos maiores, resultando potencialmente em um limite de decisão mais complicado (overfitting).\n",
    "<br>\n",
    "\n",
    "- learning_rate: Controla o tamanho da etapa na atualização dos pesos. Usado somente quando solver = 'sgd' ou 'adam'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://datascience.stackexchange.com/questions/36049/how-to-adjust-the-hyperparameters-of-mlp-classifier-to-get-more-perfect-performa\n",
    "# https://www.youtube.com/watch?v=tWKQ3Fk42yw\n",
    "# https://matheusfacure.github.io/2017/07/12/activ-func/\n",
    "# http://deeplearningbook.com.br/funcao-de-ativacao/#:~:text=ReLU%20%C3%A9%20a%20fun%C3%A7%C3%A3o%20de,neur%C3%B4nios%20ativados%20pela%20fun%C3%A7%C3%A3o%20ReLU.\n",
    "\n",
    "mlp = MLPClassifier(max_iter=500, random_state = 999)\n",
    "\n",
    "parameter_space = {\n",
    "    'hidden_layer_sizes': [(2,3,5), (10,10,10), (10,25,50), (50,50,50), (50,100,150)],\n",
    "    'activation': ['tanh', 'logistic','relu'],\n",
    "    'solver': ['sgd', 'adam'],\n",
    "    'alpha': [0.001, 0.01, 0.05],\n",
    "    'learning_rate': ['constant','adaptive'],\n",
    "}\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "clf = GridSearchCV(mlp, parameter_space, n_jobs=-1, cv=3)\n",
    "clf.fit(X_under_train[X_WOE3], y_under_train)\n",
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><strong><font color = \"darkblue\" size=4> -----------------  Redes Neurais: Dados Balanceados -------------------</font></strong></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RN_under = MLPClassifier(alpha = 0.05, hidden_layer_sizes = (10, 10, 10),\n",
    "                         activation = \"relu\", learning_rate = \"constant\", solver = \"adam\",\n",
    "                         max_iter = 1000).fit(X_under_train[features], y_under_train)\n",
    "\n",
    "\n",
    "# Train Adaboost Classifer\n",
    "rn_test_under = AVALIA_MOD(modelo = RN_under, y_prob = X_test[features], y_obs = y_test, titulo = \"Redes: Balanc\", base = \"Teste\", corte = 0.3)\n",
    "rn_val_under  = AVALIA_MOD(modelo = RN_under, y_prob = X_val[features],  y_obs = y_val,  titulo = \"Redes: Balanc\", base = \"Valida\", corte = 0.3)\n",
    "\n",
    "rn_test_under = SHIFT_AUC(x = rn_test_under, y = rn_val_under)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><strong><font color = \"darkblue\" size=4> -----------------  Redes Neurais: Dados Não Balanceados -------------------</font></strong></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RN = MLPClassifier(alpha = 0.05, hidden_layer_sizes = (10, 20, 10),\n",
    "                         activation = \"relu\", learning_rate = \"constant\", solver = \"adam\",\n",
    "                         max_iter = 1000).fit(X_train[features], y_train)\n",
    "\n",
    "# Train Adaboost Classifer\n",
    "rn_test = AVALIA_MOD(modelo = RN, y_prob = X_test[features], y_obs = y_test, titulo = \"Redes: Desbalanc\", base = \"Teste\", corte = 0.5)\n",
    "rn_val  = AVALIA_MOD(modelo = RN, y_prob = X_val[features],  y_obs = y_val,  titulo = \"Redes: Desbalanc\", base = \"Valida\", corte = 0.5)\n",
    "\n",
    "rn_test = SHIFT_AUC(x = rn_test, y = rn_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><strong><font color = \"darkblue\" size=4> -----------------  Redes Neurais: Dados WOE Não Balanceados -------------------</font></strong></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RN_WOE = MLPClassifier(alpha = 0.01, hidden_layer_sizes = (10, 10, 10),\n",
    "                         activation = \"relu\", learning_rate = \"constant\", solver = \"adam\",\n",
    "                         max_iter = 1000).fit(X_train[X_WOE3], y_train)\n",
    "\n",
    "# Train Adaboost Classifer\n",
    "rn_WOE_test = AVALIA_MOD(modelo = RN_WOE, y_prob = X_test[X_WOE3], y_obs = y_test, titulo = \"Redes: WOE Desbalanc\", base = \"Teste\", corte = np.mean(y_test))\n",
    "rn_WOE_val  = AVALIA_MOD(modelo = RN_WOE, y_prob = X_val[X_WOE3],  y_obs = y_val,  titulo = \"Redes: WOE Desbalanc\", base = \"Valida\", corte = np.mean(y_val))\n",
    "\n",
    "rn_WOE_test = SHIFT_AUC(x = rn_WOE_test, y = rn_WOE_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><strong><font color = \"darkblue\" size=4> -----------------  Redes Neurais: Dados WOE Balanceados -------------------</font></strong></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RN_under_WOE = MLPClassifier(alpha = 0.01, hidden_layer_sizes = (10, 25, 50),\n",
    "                         activation = \"logistic\", learning_rate = \"constant\", solver = \"adam\",\n",
    "                         max_iter = 1000).fit(X_under_train[X_WOE3], y_under_train)\n",
    "\n",
    "#ML_IMPORTANCIA(RN_under, features)\n",
    "\n",
    "# Train Adaboost Classifer\n",
    "rn_WOE_under_test = AVALIA_MOD(modelo = RN_under_WOE, y_prob = X_test[X_WOE3], y_obs = y_test, titulo = \"Redes: WOE Balanc\", base = \"Teste\", corte = 0.3)\n",
    "rn_WOE_under_val  = AVALIA_MOD(modelo = RN_under_WOE, y_prob = X_val[X_WOE3],  y_obs = y_val,  titulo = \"Redes: WOE Balanc\", base = \"Valida\", corte = 0.3)\n",
    "\n",
    "rn_WOE_under_test = SHIFT_AUC(x = rn_WOE_under_test, y = rn_WOE_under_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<center><strong><font color = \"darkblue\" size=4> Comparação dos modelos </font></strong></center>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdList = [\n",
    "    \n",
    "logit_under_WOE_test[2],\n",
    "logit_WOE_test[2].iloc[:,1],\n",
    "logit_under_test[2].iloc[:,1],\n",
    "logit_test[2].iloc[:,1],    \n",
    "    \n",
    "rf_test_under_WOE[2].iloc[:,1],\n",
    "rf_test_WOE[2].iloc[:,1],\n",
    "rf_test_under[2].iloc[:,1],    \n",
    "rf_test[2].iloc[:,1],\n",
    "    \n",
    "gb_under_WOE_test[2].iloc[:,1],  \n",
    "gb_WOE_test[2].iloc[:,1],\n",
    "gb_test_under[2].iloc[:,1], \n",
    "gb_test[2].iloc[:,1],\n",
    "    \n",
    "ada_WOE_test[2].iloc[:,1],\n",
    "ada_test_WOE_under[2].iloc[:,1],    \n",
    "ada_test_under[2].iloc[:,1],\n",
    "ada_test[2].iloc[:,1],\n",
    "    \n",
    "rn_WOE_under_test[2].iloc[:,1],\n",
    "rn_WOE_test[2].iloc[:,1],\n",
    "rn_test_under[2].iloc[:,1],\n",
    "rn_test[2].iloc[:,1]\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compara = pd.concat(pdList, axis=1)\n",
    "compara.columns = [\"Metricas\", \"Logit_WOE_balanc\", \"Logit_WOE\", \"Logit_balanc\", \"Logit\", \n",
    "                               \"RF_WOE_balanc\", \"RF_WOE\", \"RF_balanc\", \"RF\",\n",
    "                               \"GB_WOE_balanc\", \"GB_WOE\", \"GB_balanc\", \"GB\",\n",
    "                               \"ADA_WOE_balanc\",\"ADA_WOE\",\"ADA_balanc\",\"ADA\",\n",
    "                               \"NR_WOE_balanc\",\"NR_WOE\",\"NR_balanc\",\"NR\"\n",
    "                  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compara"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_ord = make_subplots(rows = 5, cols = 4,\n",
    "                        subplot_titles = (\n",
    "            logit_under_WOE_test[5],\n",
    "            logit_WOE_test[5],\n",
    "            logit_under_test[5],\n",
    "            logit_test[5] ,\n",
    "            rf_test_under_WOE[5],\n",
    "            rf_test_WOE[5],\n",
    "            rf_test_under[5],  \n",
    "            rf_test[5],\n",
    "            gb_under_WOE_test[5],\n",
    "            gb_WOE_test[5],\n",
    "            gb_test_under[5],\n",
    "            gb_test[5],\n",
    "            ada_test_WOE_under[5],   \n",
    "            ada_WOE_test[5],\n",
    "            ada_test_under[5],\n",
    "            ada_test[5],\n",
    "            rn_WOE_under_test[5] , \n",
    "            rn_WOE_test[5],\n",
    "            rn_test_under[5],\n",
    "            rn_test[5]                            \n",
    "))\n",
    "\n",
    "trace11 = logit_under_WOE_test[3]  \n",
    "trace12 = logit_WOE_test[3]\n",
    "trace13 = logit_under_test[3]\n",
    "trace14 = logit_test[3]  \n",
    "\n",
    "trace21 = rf_test_under_WOE[3]  \n",
    "trace22 = rf_test_WOE[3]\n",
    "trace23 = rf_test_under[3]\n",
    "trace24 = rf_test[3]  \n",
    "\n",
    "trace31 = gb_under_WOE_test[3]  \n",
    "trace32 = gb_WOE_test[3]\n",
    "trace33 = gb_test_under[3]\n",
    "trace34 = gb_test[3]  \n",
    "\n",
    "trace41 = ada_test_WOE_under[3]  \n",
    "trace42 = ada_WOE_test[3]\n",
    "trace43 = ada_test_under[3]\n",
    "trace44 = ada_test[3]  \n",
    "\n",
    "trace51 = rn_WOE_under_test[3]  \n",
    "trace52 = rn_WOE_test[3]\n",
    "trace53 = rn_test_under[3]\n",
    "trace54 = rn_test[3] \n",
    "\n",
    "plot_ord.append_trace(trace11, 1, 1)\n",
    "plot_ord.append_trace(trace12, 1, 2)\n",
    "plot_ord.append_trace(trace13, 1, 3)\n",
    "plot_ord.append_trace(trace14, 1, 4)\n",
    "\n",
    "plot_ord.append_trace(trace21, 2, 1)\n",
    "plot_ord.append_trace(trace22, 2, 2)\n",
    "plot_ord.append_trace(trace23, 2, 3)\n",
    "plot_ord.append_trace(trace24, 2, 4)\n",
    "\n",
    "plot_ord.append_trace(trace31, 3, 1)\n",
    "plot_ord.append_trace(trace32, 3, 2)\n",
    "plot_ord.append_trace(trace33, 3, 3)\n",
    "plot_ord.append_trace(trace34, 3, 4)\n",
    "\n",
    "plot_ord.append_trace(trace41, 4, 1)\n",
    "plot_ord.append_trace(trace42, 4, 2)\n",
    "plot_ord.append_trace(trace43, 4, 3)\n",
    "plot_ord.append_trace(trace44, 4, 4)\n",
    "\n",
    "plot_ord.append_trace(trace51, 5, 1)\n",
    "plot_ord.append_trace(trace52, 5, 2)\n",
    "plot_ord.append_trace(trace53, 5, 3)\n",
    "plot_ord.append_trace(trace54, 5, 4)\n",
    "\n",
    "plot_ord.update_layout(height = 800, width = 1000, showlegend = False, title=\"Ordenação por Lift\")\n",
    "plot_ord.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trace = {}\n",
    "clrs  = {}\n",
    "\n",
    "clrred  = \"#73264d\"\n",
    "clrblue = \"#00ace6\"\n",
    "\n",
    "for j in range(0,8):\n",
    "    clrs[j] = [clrred if compara.iloc[j,1:][x] == max(compara.iloc[j,1:]) else clrblue for x in range(0, 20)]\n",
    "\n",
    "clrs[7] = \"#2d2d86\" \n",
    "clrs[8] = \"#2d2d86\"  \n",
    "    \n",
    "nomes = [compara.iloc[l,0] for l in range(0,9)]  \n",
    "nomes\n",
    "fig_comp = make_subplots(rows=10, cols=1, subplot_titles = nomes)\n",
    "for k in range(0,9):\n",
    "    trace[k,0] = go.Scatter(x = compara.iloc[0,1:].reset_index().iloc[:,0], \n",
    "                            y = compara.iloc[k,1:], mode='markers',\n",
    "                             marker_size = np.repeat(20,20), marker = dict(color = clrs[k]))\n",
    "    \n",
    "    \n",
    "    fig_comp.append_trace(trace[k,0], k+1, 1)\n",
    "    \n",
    "fig_comp.update_layout(height = 2500, width = 900, showlegend = False, title=\"Medidas de qualidade dos modelos\")\n",
    "\n",
    "fig_comp.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Normalizando os dados usando Min-Max:\n",
    "compara2 = compara.iloc[[0,3,4,5,6],1:].apply(lambda x: (x - min(x))/(max(x)-min(x)), axis=1)\n",
    "aux_comp = compara2.apply(np.sum, axis=0).sort_values(ascending=False).reset_index()\n",
    "aux_comp\n",
    "\n",
    "fig = go.Figure(data=go.Scatter(x=aux_comp.iloc[:,0], y=aux_comp.iloc[:,1], mode=\"markers\",\n",
    "                               marker_size = np.repeat(30,20)))\n",
    "\n",
    "fig.update_layout(height = 400, width = 900, showlegend = False, title=\"Rank dos modelos\")\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
